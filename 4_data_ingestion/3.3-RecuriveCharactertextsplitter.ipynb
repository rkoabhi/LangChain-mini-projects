{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Splitting from Documents- RecursiveCharacter Text Splitters\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "- How the text is split: by list of characters.\n",
    "- How the chunk size is measured: by number of characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Regression.pdf', 'page': 0}, page_content='Notes and assignment on MIS-6.  \\n \\nPrepared by Prof. K.P. Soman on 15/8/19 \\nOrthogonality, Projection matrices, approximate solution and \\nRegression \\nOn coming Saturday (17th August), First one-hour, I will devote to the \\nconcept of Orthogonality and Projection matrices.  Through this topic \\non ‘regression’, you are taking a first baby step into the beautiful world \\nof ‘AI and Data Science’. \\nMathematics Enthusiasts may learn the topic from this notes before the \\nclass and submit the assignment on 18th August. Others need not be in \\npanic. You may submit it on 23rd August. With one more topic, Linear \\nalgebra for this semester is over. You will get enough time to slowly \\nmaster the concepts covered. Soon we are moving to calculus.  \\nOrthogonality and approximate solution to Ax=b \\nThe concept of orthogonality can be utilized for finding approximate \\nsolution to Ax = b if solution does not exist. A solution does not exist \\nfor Ax=b if b is not in column s pace of A. (Above statement may be \\ndifficult to visualize initially) \\nWe will start with a simple case with two columns in A. On the way to \\nsolve this problem, we will encounter a special matrix  called \\nprojection matrix with several interesting properties.  \\n \\n \\n \\n \\n \\n \\n '),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 1}, page_content='Projection matrices \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n   \\n | | |\\n|| |ˆ  or y e y\\uf02b \\uf03d \\n 1 2\\n1 2\\n2\\n1 2 a matrix  with  and  R  as columns\\n and   column space of \\nLet  R  be a vector which is not in colum n space of A  \\nThat is y is not in the plane spanned by   and  m\\nm\\nmConsider A x x\\nx x span A\\ny\\nx x\\uf0b4\\uf0ce\\n\\uf0ce\\nˆLet  be the projected vector of y in the  column space of A y\\nˆ  is expressible as a linear combination  of columns of A Now y\\n| | | | |1 1\\n1 2 1 1 2 2|| | | | 2 2ˆ=y A A x x x x\\uf061 \\uf061\\uf061 \\uf061 \\uf061\\uf061 \\uf061\\uf0e9 \\uf0f9 \\uf0e9 \\uf0f9 \\uf0e9 \\uf0f9\\uf03d \\uf03d \\uf03d \\uf02b\\uf0ea \\uf0fa \\uf0ea \\uf0fa \\uf0ea \\uf0fa\\uf0eb \\uf0fb \\uf0eb \\uf0fb \\uf0eb \\uf0fb\\n  Now A e y\\uf061\\uf02b \\uf03d\\n that  vector is orthogonal to column ve ctors of A Note e\\ne '),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 2}, page_content=' \\n \\nThis formula for P is applicable if matrix TA A  is invertible.  \\nTA A is invertible if all columns of A are independent. In the above \\nexample, the two column vectors are not collinear and hence \\nindependent. \\nIn case, all columns are not independent, above formula do not work, \\nsince ATA is not invertible. One way to solve this problem is to collect \\none set of r independent columns from A and form a matrix B, where \\nr is the rank of the matrix A. Then the new projection matrix is \\nP=B(BTA)-1 BT.  \\nThe most generic way of getting the projection matrix for projecting a \\nvector into column space of A is given by the following formula. \\n† †  where  is pseudo inverse of P A A A A\\uf03d \\uf0b4  \\nFor any matrix (square and rectangular), there is a pseudo inverse . \\nYou can appreciate this formula for projection matrix only after \\nunderstanding the concept of singular value decomposition of a \\nmatrix.  \\nSo, the easiest and safest way of finding projected vector into column \\nspace is \\n† †ˆ     ( ) y A A y AA y\\uf03d \\uf0b4 \\uf0b4 \\uf03d  \\nNote that vector y should have same tuple-size as column vectors of A. 1\\n20 0 vector0T\\nT\\nTx eA e\\nx e\\uf0e9 \\uf0f9\\uf0e9 \\uf0f9\\uf05c \\uf03d \\uf03d \\uf03d\\uf0ea \\uf0fa\\uf0ea \\uf0fa\\uf0eb \\uf0fb\\uf0eb \\uf0fb\\n\\uf028\\uf029\\n\\uf028 \\uf029 \\uf028 \\uf0291\\n1 1  \\nˆ  where \\n  for projecting a vector onto column sp aceT T T T T\\nT T T TNow A e y A A A e A y A A A y\\ny A A A A A y Py P A A A A\\nP projection matrix\\uf061 \\uf061 \\uf061\\n\\uf061\\uf02d\\n\\uf02d \\uf02d\\uf02b \\uf03d \\uf0de \\uf02b \\uf03d \\uf0de \\uf03d\\n\\uf03d \\uf03d \\uf03d \\uf03d\\n\\uf03d'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 3}, page_content='How will you project a vector z onto row space of A ?. \\nProjecting of given vector z on to rowsapce of A is same as projecting \\nz onto  column space of AT .   \\n  \\uf028 \\uf029 \\uf028 \\uf029\\uf028\\uf029† †ˆ     T T T Tz A A z A A z\\uf03d \\uf0b4 \\uf0b4 \\uf03d \\nApplication to fitting linear regression lines \\n \\n \\n \\nIn its simplest form, ‘linear regression’  is about finding the linear \\nrelationship between a dependent and an independent variable. For \\nexample, we know there is an approximate linear relationship between \\nheight and weight of individuals in a population. Higher the height of \\na person, more the weight in general he/she has. Here, weight variable \\ndepends on height variable and hence, we say, height is independent \\nvariable and weight is dependent variable. If we collect data from a \\npopulation and plot, what we obtain is a scattered collection of points \\n(not all falling on a straight line) with a linear trend as in figure given \\nbelow.  \\n \\n'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 4}, page_content=' \\n \\nWe fit what is called a regression line , i.e., a line passing through the \\ndata, the equation of which can approximately predict weight given the \\nheight of the person.  \\n'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 5}, page_content=' \\nSuch a line can be fitted using the concept of projection matrix . \\nAssume we have collected height values in column vector x and \\ncorresponding weight values in column vector y. \\nWe are trying to find a relation of the form y = mx+c  \\nWe formulate the problem as follows \\n'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 6}, page_content='1 1 1\\n2 2 2\\n3 3 3\\n4 4 4\\n5 5 5\\n1 1 1r r r\\nn n n\\nn n ny mx c e\\ny mx c e\\ny mx c e\\ny mx c e\\ny mx c e\\ny mx c e\\ny mx c e\\ny mx c e\\uf02d \\uf02d \\uf02d\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\uf04d\\n\\uf04d  \\nwhere, e1, e2,  …,  en are deviation( it can be positive or negative \\ndepending on whether the point is above or below the fitted central line) \\nfrom the y-values of the central line to be fitted.  \\nIn matrix format, above formulation is:   \\n\\uf07b\\uf07b\\n\\uf07b1 1 1\\n2 2 21\\n1\\n1n n n\\ny e Ay x e\\ny x e my A ec\\ny x e\\uf061\\uf061\\uf0e6 \\uf0f6 \\uf0e6 \\uf0f6 \\uf0e6 \\uf0f6\\n\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf0e6 \\uf0f6\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf03d \\uf02b \\uf0de \\uf03d \\uf02b \\uf0e7 \\uf0f7\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf0e8 \\uf0f8\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\n\\uf0e8 \\uf0f8 \\uf0e8 \\uf0f8 \\uf0e8 \\uf0f8\\uf04d \\uf04d \\uf04d \\uf04d\\n\\uf031 \\uf034\\uf032\\uf034 \\uf033  \\nNow we are in the same situation as in the following figure: \\nIn the figure, X1 and X2 are column vectors in A.  \\n \\n \\n '),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 7}, page_content='  \\n \\n \\n \\n \\n \\n \\n \\n \\nWe obtain \\uf061  (the slope and intercept) using the following formula \\n \\n\\uf028 \\uf029|1\\n|ˆ;   T T mA A A y y Ac\\uf061 \\uf061\\uf02d \\uf0e6 \\uf0f6\\uf03d \\uf03d \\uf03d\\uf0e7 \\uf0f7\\uf0e8 \\uf0f8  \\nPlotting   ˆ  Vs  yx  provide the required central line, which we call \\nregression line.  \\n \\nComputational Thinking. \\nHow do we generate such data and estimate slope and intercept? \\nWarning: Some concepts used here are new to you. But you need to \\nhave only some intuitive idea about the concept. This may be easily \\nobtained from Wikipedia. Make it a habit to visit wikipedia whenever \\nyou encounter a new term or concept.  \\nMethod 1 \\nSteps:  \\ne '),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 8}, page_content=\"1. We generate n equi-spaced points on a given(assumed) line like \\ny=5x+10. We get an x vector and y vector.  \\n2. Generate n random values from normal distribution with \\nappropriate standard deviation and add to y to get yd.  plot of x vs \\nyd will be a scatted set of data points. \\nMATLAB code snippet. \\nm=5; c =10 ;  % slope and intercepts  \\nx= (-5:5)';  % x is a column vector  \\ny=m*x+c; % y is a column vector  \\nn=length(x); % number of data points  \\n% let us generate n disturbing values from normal distribution with \\n% sigma=5 . That is,  standard deviation =5  \\nnoise= 5*randn(n,1);  % nx1 column vector  \\nyd=y+noise;  \\nA=[x ones(n,1)];  \\nAlpha= inv(A'*A)*A'*yd;  \\nycap=A*Alpha;  \\nplot(x,yd, '*') ;  % plot scattered data points  \\nhold on  \\nplot(x, ycap); % plot the regression line  \\nxlabel( 'independent variable x' ) \\nylabel( 'dependent variable y' ) \\nev =yd-ycap;   % error vector.  \\n% Verify that error vector is orthogonal to column vectors of A  \\nCheck = A'*ev ;  \\n% print check on screen  \\nFormat bank  \\nCheck  \\nMethod 2 .  \\nWe can generate data from bivariate Normal distribution. (get some \\nintuitive idea from Wikipedia  about ‘normal probability distribution’, \\n‘correlation coefficient’, ‘variance’ and ‘variance-covariance matrix’)  \\nLet us consider height and weight distribution of human population. \\nLet the height be normally distributed random variable with mean 160 \\nand variance=9. \\nLet the weight be normally distributed random variable with mean 70 \\nand variance=16. \"),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 9}, page_content=\"Let the correlation coefficient \\uf072 between height and weight variable \\nbe 0.8. The correlation coefficient is a number which can take a value \\nbetween -1 and 1. We can have positive or negative correlation. \\nCovariance between height and weight variable is given by the formula  \\n( , ) ( ) ( )X Y COV X Y Var X Var Y \\uf072\\uf073 \\uf073 \\uf072\\uf03d \\uf03d   \\nWe can use above data to generate required ‘scattered data points.  \\n \\nHere is the MATLAB code. \\nclear all \\nMu=[160 70];   % mean values of height and weight variables X and Y.  \\nVarX=9; VarY=16;  % variance of X and Y variables  \\nCorC=0.8;  % Correlation Coefficient between X and Y  \\nCoVarXY=CorC*sqrt(VarX)*sqrt(VarY);  Covariance between  X and Y \\nSigma=[VarX  CoVarXY;  CoVarXY VarY ];  % Covariance matrix  \\n% Generate 500 datapoints  \\nN=500; \\nData=mvnrnd(Mu,Sigma,N); % variables X and Y are in columns  \\nplot(Data(:,1),Data(:,2), '.') \\nxlabel( 'independent variable HEIGHT' ) \\nylabel( 'dependent variable WEIGHT' ) \\n \\nClustering and Classification.  \\nRead or see videos about what is clustering  and classification  in \\n‘machine learning’ from internet.  \\nCreating data sets for clustering and classification \\nMu1= [ 5 6]; Mu2= [ 25 10]; Mu3= [15, 20];  \\nSigma=2*eye (2);  \\nN=50;  \\nclust1=mvnrnd(Mu1, Sigma,N);  \\nclust2=mvnrnd(Mu2,Sigma,N);  \\nclust3=mvnrnd(Mu3,Sigma,N);  \\nplot(clust1(:,1),clust1(:,2), 'bd') \"),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 10}, page_content=\"hold on  \\nplot(clust2(:,1),clust2(:,2), 'rv') \\nhold on  \\nplot(clust3(:,1),clust3(:,2), 'c^') \\n \\n \\n \\n Assignment Questions. It is a very \\nsimple Assignment. May take only 1 hour.  \\n \\n1. Create projection matrix P of size \\n5x5 from matrix A with size 5x2. \\n2. Find Rank of above matrix P \\n3. Find P2. What you observe? Can you \\nprove the result analytically? \\n4. Find Eigenvalue of P. What you \\nObserve? Verify the result by creating \\na new P from a new matrix A.  \\n\"),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 11}, page_content='How do you relate rank and your \\nobservation, with regards to nonzero \\neigen values?  \\n \\nFast learners may submit it on 18/9/19. \\nFor others, submission date is on or before \\n23/9/19. \\n ')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading a PDf File\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('Regression.pdf')\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to recursively split text by characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Regression.pdf', 'page': 0}, page_content='Notes and assignment on MIS-6.  \\n \\nPrepared by Prof. K.P. Soman on 15/8/19 \\nOrthogonality, Projection matrices, approximate solution and \\nRegression \\nOn coming Saturday (17th August), First one-hour, I will devote to the \\nconcept of Orthogonality and Projection matrices.  Through this topic \\non ‘regression’, you are taking a first baby step into the beautiful world \\nof ‘AI and Data Science’. \\nMathematics Enthusiasts may learn the topic from this notes before the'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 0}, page_content='class and submit the assignment on 18th August. Others need not be in \\npanic. You may submit it on 23rd August. With one more topic, Linear \\nalgebra for this semester is over. You will get enough time to slowly \\nmaster the concepts covered. Soon we are moving to calculus.  \\nOrthogonality and approximate solution to Ax=b \\nThe concept of orthogonality can be utilized for finding approximate \\nsolution to Ax = b if solution does not exist. A solution does not exist'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 0}, page_content='for Ax=b if b is not in column s pace of A. (Above statement may be \\ndifficult to visualize initially) \\nWe will start with a simple case with two columns in A. On the way to \\nsolve this problem, we will encounter a special matrix  called \\nprojection matrix with several interesting properties.'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 1}, page_content='Projection matrices \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n   \\n | | |\\n|| |ˆ  or y e y\\uf02b \\uf03d \\n 1 2\\n1 2\\n2\\n1 2 a matrix  with  and  R  as columns\\n and   column space of \\nLet  R  be a vector which is not in colum n space of A  \\nThat is y is not in the plane spanned by   and  m\\nm\\nmConsider A x x\\nx x span A\\ny\\nx x\\uf0b4\\uf0ce\\n\\uf0ce\\nˆLet  be the projected vector of y in the  column space of A y\\nˆ  is expressible as a linear combination  of columns of A Now y\\n| | | | |1 1'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 1}, page_content='| | | | |1 1\\n1 2 1 1 2 2|| | | | 2 2ˆ=y A A x x x x\\uf061 \\uf061\\uf061 \\uf061 \\uf061\\uf061 \\uf061\\uf0e9 \\uf0f9 \\uf0e9 \\uf0f9 \\uf0e9 \\uf0f9\\uf03d \\uf03d \\uf03d \\uf02b\\uf0ea \\uf0fa \\uf0ea \\uf0fa \\uf0ea \\uf0fa\\uf0eb \\uf0fb \\uf0eb \\uf0fb \\uf0eb \\uf0fb\\n  Now A e y\\uf061\\uf02b \\uf03d\\n that  vector is orthogonal to column ve ctors of A Note e\\ne'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 2}, page_content='This formula for P is applicable if matrix TA A  is invertible.  \\nTA A is invertible if all columns of A are independent. In the above \\nexample, the two column vectors are not collinear and hence \\nindependent. \\nIn case, all columns are not independent, above formula do not work, \\nsince ATA is not invertible. One way to solve this problem is to collect \\none set of r independent columns from A and form a matrix B, where \\nr is the rank of the matrix A. Then the new projection matrix is'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 2}, page_content='P=B(BTA)-1 BT.  \\nThe most generic way of getting the projection matrix for projecting a \\nvector into column space of A is given by the following formula. \\n† †  where  is pseudo inverse of P A A A A\\uf03d \\uf0b4  \\nFor any matrix (square and rectangular), there is a pseudo inverse . \\nYou can appreciate this formula for projection matrix only after \\nunderstanding the concept of singular value decomposition of a \\nmatrix.  \\nSo, the easiest and safest way of finding projected vector into column \\nspace is'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 2}, page_content='space is \\n† †ˆ     ( ) y A A y AA y\\uf03d \\uf0b4 \\uf0b4 \\uf03d  \\nNote that vector y should have same tuple-size as column vectors of A. 1\\n20 0 vector0T\\nT\\nTx eA e\\nx e\\uf0e9 \\uf0f9\\uf0e9 \\uf0f9\\uf05c \\uf03d \\uf03d \\uf03d\\uf0ea \\uf0fa\\uf0ea \\uf0fa\\uf0eb \\uf0fb\\uf0eb \\uf0fb\\n\\uf028\\uf029\\n\\uf028 \\uf029 \\uf028 \\uf0291\\n1 1  \\nˆ  where \\n  for projecting a vector onto column sp aceT T T T T\\nT T T TNow A e y A A A e A y A A A y\\ny A A A A A y Py P A A A A\\nP projection matrix\\uf061 \\uf061 \\uf061\\n\\uf061\\uf02d\\n\\uf02d \\uf02d\\uf02b \\uf03d \\uf0de \\uf02b \\uf03d \\uf0de \\uf03d\\n\\uf03d \\uf03d \\uf03d \\uf03d\\n\\uf03d'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 3}, page_content='How will you project a vector z onto row space of A ?. \\nProjecting of given vector z on to rowsapce of A is same as projecting \\nz onto  column space of AT .   \\n  \\uf028 \\uf029 \\uf028 \\uf029\\uf028\\uf029† †ˆ     T T T Tz A A z A A z\\uf03d \\uf0b4 \\uf0b4 \\uf03d \\nApplication to fitting linear regression lines \\n \\n \\n \\nIn its simplest form, ‘linear regression’  is about finding the linear \\nrelationship between a dependent and an independent variable. For \\nexample, we know there is an approximate linear relationship between'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 3}, page_content='height and weight of individuals in a population. Higher the height of \\na person, more the weight in general he/she has. Here, weight variable \\ndepends on height variable and hence, we say, height is independent \\nvariable and weight is dependent variable. If we collect data from a \\npopulation and plot, what we obtain is a scattered collection of points \\n(not all falling on a straight line) with a linear trend as in figure given \\nbelow.'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 4}, page_content='We fit what is called a regression line , i.e., a line passing through the \\ndata, the equation of which can approximately predict weight given the \\nheight of the person.'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 5}, page_content='Such a line can be fitted using the concept of projection matrix . \\nAssume we have collected height values in column vector x and \\ncorresponding weight values in column vector y. \\nWe are trying to find a relation of the form y = mx+c  \\nWe formulate the problem as follows'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 6}, page_content='1 1 1\\n2 2 2\\n3 3 3\\n4 4 4\\n5 5 5\\n1 1 1r r r\\nn n n\\nn n ny mx c e\\ny mx c e\\ny mx c e\\ny mx c e\\ny mx c e\\ny mx c e\\ny mx c e\\ny mx c e\\uf02d \\uf02d \\uf02d\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\n\\uf03d \\uf02b \\uf02b\\uf04d\\n\\uf04d  \\nwhere, e1, e2,  …,  en are deviation( it can be positive or negative \\ndepending on whether the point is above or below the fitted central line) \\nfrom the y-values of the central line to be fitted.  \\nIn matrix format, above formulation is:   \\n\\uf07b\\uf07b\\n\\uf07b1 1 1\\n2 2 21\\n1\\n1n n n\\ny e Ay x e\\ny x e my A ec\\ny x e\\uf061\\uf061\\uf0e6 \\uf0f6 \\uf0e6 \\uf0f6 \\uf0e6 \\uf0f6'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 6}, page_content='y e Ay x e\\ny x e my A ec\\ny x e\\uf061\\uf061\\uf0e6 \\uf0f6 \\uf0e6 \\uf0f6 \\uf0e6 \\uf0f6\\n\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf0e6 \\uf0f6\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf03d \\uf02b \\uf0de \\uf03d \\uf02b \\uf0e7 \\uf0f7\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\uf0e8 \\uf0f8\\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7 \\uf0e7 \\uf0f7\\n\\uf0e8 \\uf0f8 \\uf0e8 \\uf0f8 \\uf0e8 \\uf0f8\\uf04d \\uf04d \\uf04d \\uf04d\\n\\uf031 \\uf034\\uf032\\uf034 \\uf033  \\nNow we are in the same situation as in the following figure: \\nIn the figure, X1 and X2 are column vectors in A.'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 7}, page_content='We obtain \\uf061  (the slope and intercept) using the following formula \\n \\n\\uf028 \\uf029|1\\n|ˆ;   T T mA A A y y Ac\\uf061 \\uf061\\uf02d \\uf0e6 \\uf0f6\\uf03d \\uf03d \\uf03d\\uf0e7 \\uf0f7\\uf0e8 \\uf0f8  \\nPlotting   ˆ  Vs  yx  provide the required central line, which we call \\nregression line.  \\n \\nComputational Thinking. \\nHow do we generate such data and estimate slope and intercept? \\nWarning: Some concepts used here are new to you. But you need to \\nhave only some intuitive idea about the concept. This may be easily'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 7}, page_content='obtained from Wikipedia. Make it a habit to visit wikipedia whenever \\nyou encounter a new term or concept.  \\nMethod 1 \\nSteps:  \\ne'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 8}, page_content=\"1. We generate n equi-spaced points on a given(assumed) line like \\ny=5x+10. We get an x vector and y vector.  \\n2. Generate n random values from normal distribution with \\nappropriate standard deviation and add to y to get yd.  plot of x vs \\nyd will be a scatted set of data points. \\nMATLAB code snippet. \\nm=5; c =10 ;  % slope and intercepts  \\nx= (-5:5)';  % x is a column vector  \\ny=m*x+c; % y is a column vector  \\nn=length(x); % number of data points\"),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 8}, page_content=\"n=length(x); % number of data points  \\n% let us generate n disturbing values from normal distribution with \\n% sigma=5 . That is,  standard deviation =5  \\nnoise= 5*randn(n,1);  % nx1 column vector  \\nyd=y+noise;  \\nA=[x ones(n,1)];  \\nAlpha= inv(A'*A)*A'*yd;  \\nycap=A*Alpha;  \\nplot(x,yd, '*') ;  % plot scattered data points  \\nhold on  \\nplot(x, ycap); % plot the regression line  \\nxlabel( 'independent variable x' ) \\nylabel( 'dependent variable y' ) \\nev =yd-ycap;   % error vector.\"),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 8}, page_content=\"ev =yd-ycap;   % error vector.  \\n% Verify that error vector is orthogonal to column vectors of A  \\nCheck = A'*ev ;  \\n% print check on screen  \\nFormat bank  \\nCheck  \\nMethod 2 .  \\nWe can generate data from bivariate Normal distribution. (get some \\nintuitive idea from Wikipedia  about ‘normal probability distribution’, \\n‘correlation coefficient’, ‘variance’ and ‘variance-covariance matrix’)  \\nLet us consider height and weight distribution of human population.\"),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 8}, page_content='Let the height be normally distributed random variable with mean 160 \\nand variance=9. \\nLet the weight be normally distributed random variable with mean 70 \\nand variance=16.'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 9}, page_content='Let the correlation coefficient \\uf072 between height and weight variable \\nbe 0.8. The correlation coefficient is a number which can take a value \\nbetween -1 and 1. We can have positive or negative correlation. \\nCovariance between height and weight variable is given by the formula  \\n( , ) ( ) ( )X Y COV X Y Var X Var Y \\uf072\\uf073 \\uf073 \\uf072\\uf03d \\uf03d   \\nWe can use above data to generate required ‘scattered data points.  \\n \\nHere is the MATLAB code. \\nclear all'),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 9}, page_content=\"Here is the MATLAB code. \\nclear all \\nMu=[160 70];   % mean values of height and weight variables X and Y.  \\nVarX=9; VarY=16;  % variance of X and Y variables  \\nCorC=0.8;  % Correlation Coefficient between X and Y  \\nCoVarXY=CorC*sqrt(VarX)*sqrt(VarY);  Covariance between  X and Y \\nSigma=[VarX  CoVarXY;  CoVarXY VarY ];  % Covariance matrix  \\n% Generate 500 datapoints  \\nN=500; \\nData=mvnrnd(Mu,Sigma,N); % variables X and Y are in columns  \\nplot(Data(:,1),Data(:,2), '.')\"),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 9}, page_content=\"plot(Data(:,1),Data(:,2), '.') \\nxlabel( 'independent variable HEIGHT' ) \\nylabel( 'dependent variable WEIGHT' ) \\n \\nClustering and Classification.  \\nRead or see videos about what is clustering  and classification  in \\n‘machine learning’ from internet.  \\nCreating data sets for clustering and classification \\nMu1= [ 5 6]; Mu2= [ 25 10]; Mu3= [15, 20];  \\nSigma=2*eye (2);  \\nN=50;  \\nclust1=mvnrnd(Mu1, Sigma,N);  \\nclust2=mvnrnd(Mu2,Sigma,N);  \\nclust3=mvnrnd(Mu3,Sigma,N);\"),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 9}, page_content=\"clust3=mvnrnd(Mu3,Sigma,N);  \\nplot(clust1(:,1),clust1(:,2), 'bd')\"),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 10}, page_content=\"hold on  \\nplot(clust2(:,1),clust2(:,2), 'rv') \\nhold on  \\nplot(clust3(:,1),clust3(:,2), 'c^') \\n \\n \\n \\n Assignment Questions. It is a very \\nsimple Assignment. May take only 1 hour.  \\n \\n1. Create projection matrix P of size \\n5x5 from matrix A with size 5x2. \\n2. Find Rank of above matrix P \\n3. Find P2. What you observe? Can you \\nprove the result analytically? \\n4. Find Eigenvalue of P. What you \\nObserve? Verify the result by creating \\na new P from a new matrix A.\"),\n",
       " Document(metadata={'source': 'Regression.pdf', 'page': 11}, page_content='How do you relate rank and your \\nobservation, with regards to nonzero \\neigen values?  \\n \\nFast learners may submit it on 18/9/19. \\nFor others, submission date is on or before \\n23/9/19.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "final_documents=text_splitter.split_documents(docs)\n",
    "final_documents\n",
    "#split into chunks of 500 characters and there can overlap of 5o characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Notes and assignment on MIS-6.  \n",
      " \n",
      "Prepared by Prof. K.P. Soman on 15/8/19 \n",
      "Orthogonality, Projection matrices, approximate solution and \n",
      "Regression \n",
      "On coming Saturday (17th August), First one-hour, I will devote to the \n",
      "concept of Orthogonality and Projection matrices.  Through this topic \n",
      "on ‘regression’, you are taking a first baby step into the beautiful world \n",
      "of ‘AI and Data Science’. \n",
      "Mathematics Enthusiasts may learn the topic from this notes before the' metadata={'source': 'Regression.pdf', 'page': 0}\n",
      "page_content='class and submit the assignment on 18th August. Others need not be in \n",
      "panic. You may submit it on 23rd August. With one more topic, Linear \n",
      "algebra for this semester is over. You will get enough time to slowly \n",
      "master the concepts covered. Soon we are moving to calculus.  \n",
      "Orthogonality and approximate solution to Ax=b \n",
      "The concept of orthogonality can be utilized for finding approximate \n",
      "solution to Ax = b if solution does not exist. A solution does not exist' metadata={'source': 'Regression.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(final_documents[0])\n",
    "print(final_documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text Loader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader('speech.txt')\n",
    "docs=loader.load()\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of'\n",
      "page_content='foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no'\n"
     ]
    }
   ],
   "source": [
    "speech=\"\"\n",
    "with open(\"speech.txt\") as f:\n",
    "    speech=f.read()\n",
    "\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)\n",
    "text=text_splitter.create_documents([speech])\n",
    "print(text[0])\n",
    "print(text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we see above sentence repition is there , this is becuasae of chunk_overlap parameter used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
